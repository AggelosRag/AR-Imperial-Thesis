{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "from torchvision import datasets, transforms"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T09:09:48.460788Z",
     "start_time": "2024-08-07T09:09:47.348918Z"
    }
   },
   "id": "4777cff0e00efc14"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "'/Users/gouse/PycharmProjects/AR-Imperial-Thesis'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('/Users/gouse/PycharmProjects/AR-Imperial-Thesis')\n",
    "os.getcwd()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T09:09:48.463159Z",
     "start_time": "2024-08-07T09:09:48.460985Z"
    }
   },
   "id": "d12042f801e5db90"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load the data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "23f23e1d3374fdcc"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "feature_names = [\"thickness_small\", \"thickness_medium\", \"thickness_large\", \"thickness_xlarge\",\n",
    "                 \"width_small\", \"width_medium\", \"width_large\", \"width_xlarge\",\n",
    "                 \"length_small\", \"length_medium\", \"length_large\", \"length_xlarge\"]\n",
    "\n",
    "class_names = [\"6\", \"8\", \"9\"]\n",
    "\n",
    "# mapping from feature index to feature name\n",
    "feature_index_to_name = {i: feature_name for i, feature_name in enumerate(feature_names)}\n",
    "# mapping from feature name to feature index\n",
    "feature_name_to_index = {feature_name: i for i, feature_name in enumerate(feature_names)}\n",
    "# mapping from class index to class name\n",
    "class_index_to_name = {i: class_name for i, class_name in enumerate(class_names)}\n",
    "# mapping from class name to class index\n",
    "class_name_to_index = {class_name: i for i, class_name in enumerate(class_names)}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T09:09:48.467779Z",
     "start_time": "2024-08-07T09:09:48.464098Z"
    }
   },
   "id": "399b92d37f5172d7"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Download training and test sets\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize the images\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./datasets/MNIST/data', train=True,\n",
    "                               download=True,\n",
    "                               transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./datasets/MNIST/data', train=False,\n",
    "                              download=True,\n",
    "                              transform=transform)\n",
    "\n",
    "dict_of_lists = {6: [], 8: [], 9: []}\n",
    "for i, (_, label) in enumerate(train_dataset):\n",
    "    if label in dict_of_lists.keys():\n",
    "        dict_of_lists[label].append(\n",
    "            train_dataset.data[i].reshape(1, 28, 28))\n",
    "\n",
    "for key in dict_of_lists.keys():\n",
    "    dict_of_lists[key] = np.vstack(dict_of_lists[key]).reshape(-1, 1,\n",
    "                                                               28, 28)\n",
    "    if key == 8:\n",
    "        X = torch.cat((torch.tensor(dict_of_lists[6]),\n",
    "                       torch.tensor(dict_of_lists[8])))\n",
    "    elif key > 8:\n",
    "        X = torch.cat((X, torch.tensor(dict_of_lists[key])))\n",
    "\n",
    "# import pickle files\n",
    "with open('./datasets/MNIST/mine_preprocessed/area_dict.pkl', 'rb') as f:\n",
    "    area = pickle.load(f)\n",
    "with open('./datasets/MNIST/mine_preprocessed/length_dict.pkl', 'rb') as f:\n",
    "    length = pickle.load(f)\n",
    "with open('./datasets/MNIST/mine_preprocessed/thickness_dict.pkl', 'rb') as f:\n",
    "    thickness = pickle.load(f)\n",
    "with open('./datasets/MNIST/mine_preprocessed/slant_dict.pkl', 'rb') as f:\n",
    "    slant = pickle.load(f)\n",
    "with open('./datasets/MNIST/mine_preprocessed/width_dict.pkl', 'rb') as f:\n",
    "    width = pickle.load(f)\n",
    "with open('./datasets/MNIST/mine_preprocessed/height_dict.pkl', 'rb') as f:\n",
    "    height = pickle.load(f)\n",
    "\n",
    "# load the targets test\n",
    "with open('./datasets/MNIST/mine_preprocessed/area_dict_test.pkl', 'rb') as f:\n",
    "    area_test = pickle.load(f)\n",
    "with open('./datasets/MNIST/mine_preprocessed/length_dict_test.pkl', 'rb') as f:\n",
    "    length_test = pickle.load(f)\n",
    "with open('./datasets/MNIST/mine_preprocessed/thickness_dict_test.pkl', 'rb') as f:\n",
    "    thickness_test = pickle.load(f)\n",
    "with open('./datasets/MNIST/mine_preprocessed/slant_dict_test.pkl', 'rb') as f:\n",
    "    slant_test = pickle.load(f)\n",
    "with open('./datasets/MNIST/mine_preprocessed/width_dict_test.pkl', 'rb') as f:\n",
    "    width_test = pickle.load(f)\n",
    "with open('./datasets/MNIST/mine_preprocessed/height_dict_test.pkl', 'rb') as f:\n",
    "    height_test = pickle.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T09:09:50.854141Z",
     "start_time": "2024-08-07T09:09:48.470010Z"
    }
   },
   "id": "f3accd7b5d74fd5f"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 0\n",
      "  Bin 1: Min = 1.0608199852609512, Max = 2.0955714117145585\n",
      "    Closest to Min: [ 5950  6513 16167  1617  8621]\n",
      "    Closest to Max: [ 5724 15599 12669 15554 15557]\n",
      "  Bin 2: Min = 2.095583355223437, Max = 2.455555185927587\n",
      "    Closest to Min: [  157 11735  2878 11170  5908]\n",
      "    Closest to Max: [10186    92 12276  7142 11892]\n",
      "  Bin 3: Min = 2.455627281914512, Max = 2.901642846528742\n",
      "    Closest to Min: [9156 6174 7889 4619 3771]\n",
      "    Closest to Max: [  244   426  4118  5549 12352]\n",
      "  Bin 4: Min = 2.9016807521673793, Max = 9.53389237525011\n",
      "    Closest to Min: [17317  8534  9637 12831 12134]\n",
      "    Closest to Max: [ 6880 11069  6466  8310 10537]\n",
      "Feature 1\n",
      "  Bin 1: Min = 5.381124287425585, Max = 10.939789083379203\n",
      "    Closest to Min: [12129 10069  1848  2104  1426]\n",
      "    Closest to Max: [  295 14518 15892 14841 10100]\n",
      "  Bin 2: Min = 10.940738953737265, Max = 12.576095107156878\n",
      "    Closest to Min: [11208  4297  6288  6421 12348]\n",
      "    Closest to Max: [11690 16675 17656 10882 16855]\n",
      "  Bin 3: Min = 12.57627552105846, Max = 14.53266088562656\n",
      "    Closest to Min: [10795  1230   974 17043 16225]\n",
      "    Closest to Max: [ 4033 16861 15528  2335  2363]\n",
      "  Bin 4: Min = 14.53321565083615, Max = 21.116715020198313\n",
      "    Closest to Min: [ 1265 14605 16776 17406  3546]\n",
      "    Closest to Max: [15279  4458  2270 13351 15146]\n",
      "Feature 2\n",
      "  Bin 1: Min = 15.010407640085656, Max = 42.074116139070405\n",
      "    Closest to Min: [14013 11069  1045  2741  1559]\n",
      "    Closest to Max: [17097  2415 12566  2643  1360]\n",
      "  Bin 2: Min = 42.07716446627535, Max = 47.85229073212243\n",
      "    Closest to Min: [ 4615  2160 16288  8562 15961]\n",
      "    Closest to Max: [2148 4471 3860 2912 3597]\n",
      "  Bin 3: Min = 47.85965004500313, Max = 54.46625176280135\n",
      "    Closest to Min: [ 3554 12021  7628 15006  3844]\n",
      "    Closest to Max: [10118  7882  1385  6383 10381]\n",
      "  Bin 4: Min = 54.47361107568206, Max = 93.52133829043453\n",
      "    Closest to Min: [ 3400  3818 10753  2259  5021]\n",
      "    Closest to Max: [ 2884 10182 10668  8408  6162]\n"
     ]
    }
   ],
   "source": [
    "targets = []\n",
    "digits_size = 0\n",
    "labels = []\n",
    "# for i in range(4,10):\n",
    "for i in [6, 8, 9]:\n",
    "    # targets += list(\n",
    "    #     zip(thickness[i], width[i], slant[i], height[i]))\n",
    "    targets += list(\n",
    "        zip(thickness[i], width[i], length[i]))\n",
    "    # targets += list(\n",
    "    # zip(thickness[i], area[i], length[i],\n",
    "    #                     width[i], height[i], slant[i]))\n",
    "    if i == 6:\n",
    "        k = 0\n",
    "    elif i == 8:\n",
    "        k = 1\n",
    "    else:\n",
    "        k = 2\n",
    "    # labels.append([(i-4) for j in range(len(targets) - digits_size)])\n",
    "    labels.append([k for j in range(len(targets) - digits_size)])\n",
    "    digits_size += len(width[i])\n",
    "\n",
    "targets = np.array(targets)\n",
    "\n",
    "def assign_bins(data, bin_edges):\n",
    "    return np.digitize(data, bins=bin_edges, right=True)\n",
    "\n",
    "# Convert bin numbers to one-hot encoded values\n",
    "def one_hot_encode(bin_numbers, num_bins):\n",
    "    return np.eye(num_bins)[bin_numbers - 1]\n",
    "\n",
    "def process_data(targets, num_bins=4):\n",
    "    bins_data_all_indices = {}\n",
    "    bins_data_all = []\n",
    "    min_max_values_all = []\n",
    "    closest_images_all = []\n",
    "    bin_counts = []\n",
    "\n",
    "    for i in range(targets.shape[1]):\n",
    "        # Combine the data\n",
    "        combined_data = list(targets[:, i])\n",
    "\n",
    "        # Sort the combined data\n",
    "        combined_sorted = np.sort(combined_data)\n",
    "\n",
    "        # Determine the number of data points per bin\n",
    "        bin_size = len(combined_sorted) // num_bins\n",
    "\n",
    "        # Calculate bin edges\n",
    "        bin_edges = [combined_sorted[i * bin_size] for i in range(1, num_bins)] + [combined_sorted[-1]]\n",
    "        bin_edges = [-np.inf] + bin_edges\n",
    "\n",
    "        # Assign bins to the original data lists\n",
    "        bins_data = assign_bins(targets[:, i], bin_edges)\n",
    "\n",
    "        # Do one-hot encoding in the bins\n",
    "        bins_data_encoded = one_hot_encode(bins_data, num_bins)\n",
    "\n",
    "        # Get min and max values per bin\n",
    "        min_max_values = []\n",
    "        closest_images = []\n",
    "        counts = []\n",
    "\n",
    "        feature_bins_data = {}\n",
    "\n",
    "        for bin_num in range(1, num_bins + 1):\n",
    "            bin_indices = np.where(bins_data == bin_num)[0]\n",
    "            bin_values = targets[bin_indices, i]\n",
    "            counts.append(len(bin_indices))\n",
    "\n",
    "            if len(bin_values) > 0:\n",
    "                min_val = np.min(bin_values)\n",
    "                max_val = np.max(bin_values)\n",
    "                min_max_values.append((min_val, max_val))\n",
    "\n",
    "                # Select 5 images closest to the minimum and 5 closest to the maximum\n",
    "                closest_min_indices = bin_indices[np.argsort(np.abs(bin_values - min_val))[:5]]\n",
    "                closest_max_indices = bin_indices[np.argsort(np.abs(bin_values - max_val))[:5]]\n",
    "                closest_images.append((closest_min_indices, closest_max_indices))\n",
    "            else:\n",
    "                min_max_values.append((None, None))\n",
    "                closest_images.append(([], []))\n",
    "                \n",
    "            feature_bins_data[bin_num] = list(bin_indices)\n",
    "\n",
    "        bins_data_all.append(bins_data_encoded)\n",
    "        bins_data_all_indices[i] = feature_bins_data\n",
    "        min_max_values_all.append(min_max_values)\n",
    "        closest_images_all.append(closest_images)\n",
    "        bin_counts.append(counts)\n",
    "\n",
    "    return bins_data_all, bins_data_all_indices, min_max_values_all, closest_images_all, bin_counts\n",
    "\n",
    "# Example usage:\n",
    "#targets = np.random.randn(100, 2)  # Example targets with 2 features and 100 samples\n",
    "num_bins = 4\n",
    "bins_data_all, bins_data_all_indices, min_max_values_all, closest_images_all, bin_counts = process_data(targets, num_bins=num_bins)\n",
    "\n",
    "# Output the results\n",
    "for feature_idx in range(targets.shape[1]):\n",
    "    print(f\"Feature {feature_idx}\")\n",
    "    for bin_idx, (min_val, max_val) in enumerate(min_max_values_all[feature_idx]):\n",
    "        print(f\"  Bin {bin_idx + 1}: Min = {min_val}, Max = {max_val}\")\n",
    "        print(f\"    Closest to Min: {closest_images_all[feature_idx][bin_idx][0]}\")\n",
    "        print(f\"    Closest to Max: {closest_images_all[feature_idx][bin_idx][1]}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T09:09:50.880296Z",
     "start_time": "2024-08-07T09:09:50.859436Z"
    }
   },
   "id": "57408da9e0c603b"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s6/pzn2mzln089b14702jlw7cqm0000gn/T/ipykernel_1656/2629746947.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(X_train, dtype=torch.float32)\n",
      "/var/folders/s6/pzn2mzln089b14702jlw7cqm0000gn/T/ipykernel_1656/2629746947.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_val = torch.tensor(X_val, dtype=torch.float32)\n",
      "/var/folders/s6/pzn2mzln089b14702jlw7cqm0000gn/T/ipykernel_1656/2629746947.py:60: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test = torch.tensor(X_test, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "C = np.stack(bins_data_all, axis=1).reshape(-1, num_bins* targets.shape[1])\n",
    "y = np.array([item for sublist in labels for item in sublist])\n",
    "np.random.seed(42)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Split the data\n",
    "def train_test_split_with_indices(*arrays, **options):\n",
    "    # Extract the test_size and train_size parameters if they exist\n",
    "    test_size = options.pop('test_size', None)\n",
    "    train_size = options.pop('train_size', None)\n",
    "    random_state = options.pop('random_state', None)\n",
    "    shuffle = options.pop('shuffle', True)\n",
    "    stratify = options.pop('stratify', None)\n",
    "    indices = options.pop('indices', None)\n",
    "\n",
    "    # Get the number of samples in the input arrays\n",
    "    n_samples = arrays[0].shape[0]\n",
    "\n",
    "    # Use provided indices or generate default indices\n",
    "    if indices is None:\n",
    "        indices = np.arange(n_samples)\n",
    "    \n",
    "    # Generate indices for the split\n",
    "    train_indices, test_indices = train_test_split(\n",
    "        indices, test_size=test_size, train_size=train_size, \n",
    "        random_state=random_state, shuffle=shuffle, stratify=stratify\n",
    "    )\n",
    "\n",
    "    # Split the arrays using the generated indices\n",
    "    result = []\n",
    "    for array in arrays:\n",
    "        result.append(array[train_indices])\n",
    "        result.append(array[test_indices])\n",
    "\n",
    "    # Append the indices to the result\n",
    "    result.append(train_indices)\n",
    "    result.append(test_indices)\n",
    "\n",
    "    return result\n",
    "\n",
    "X_train, X_val, C_train, C_val, y_train, y_val, train_indices, val_indices = train_test_split_with_indices(X, C, y,\n",
    "                                                                  test_size=0.5,\n",
    "                                                                  random_state=42)\n",
    "\n",
    "train_index_to_or_index = {i: original_idx for i, original_idx in enumerate(train_indices)}\n",
    "train_or_index_to_index = {original_idx: i for i, original_idx in enumerate(train_indices)}\n",
    "\n",
    "X_val, X_test, C_val, C_test, y_val, y_test, val_indices, test_indices = train_test_split_with_indices(X_val, C_val, y_val,\n",
    "                                                                  test_size=0.5,\n",
    "                                                                  random_state=42)\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "C_train = torch.tensor(C_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "C_val = torch.tensor(C_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "C_test = torch.tensor(C_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T09:09:50.988389Z",
     "start_time": "2024-08-07T09:09:50.885869Z"
    }
   },
   "id": "938ff1cf5610d467"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Load test accuracies per leaf"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e0545ad565990499"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "output_path = \"/Users/gouse/PycharmProjects/AR-Imperial-Thesis/logs_and_models_to_show/analysis_notebooks/completeness_scores/CBM_model\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T09:09:50.993013Z",
     "start_time": "2024-08-07T09:09:50.988650Z"
    }
   },
   "id": "20d71ccf6e077759"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "with open(os.path.join(output_path, 'accuracy_per_original_path_dict.pkl'), 'rb') as f:\n",
    "    accuracy_per_original_path_dict = pkl.load(f)\n",
    "with open(os.path.join(output_path, 'accuracy_per_new_path_dict.pkl'), 'rb') as f:\n",
    "    accuracy_per_new_path_dict = pkl.load(f)\n",
    "with open(os.path.join(output_path, 'leaf_samples_indices.pkl'), 'rb') as f:\n",
    "    leaf_samples_indices = pkl.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T09:09:50.997405Z",
     "start_time": "2024-08-07T09:09:50.991302Z"
    }
   },
   "id": "3a826de8ec3dacd7"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "{4: 0.5616883116883117,\n 6: 0.5977653631284916,\n 7: 0.5506445672191529,\n 10: 0.452914798206278,\n 11: 0.4608433734939759,\n 12: 0.6384615384615384,\n 15: 0.5158959537572254,\n 16: 0.7111111111111111,\n 17: 0.9809523809523809,\n 18: 0.7721280602636534}"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_per_original_path_dict"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T09:09:50.998585Z",
     "start_time": "2024-08-07T09:09:50.995335Z"
    }
   },
   "id": "9f1fc8937de3f943"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "{4: 0.5887445887445888,\n 6: 0.5977653631284916,\n 7: 0.5506445672191529,\n 10: 0.452914798206278,\n 11: 0.4608433734939759,\n 12: 0.6384615384615384,\n 15: 0.5375722543352601,\n 16: 0.7111111111111111,\n 17: 0.9809523809523809,\n 18: 0.8069679849340866}"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_per_new_path_dict"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T09:09:51.003239Z",
     "start_time": "2024-08-07T09:09:50.998764Z"
    }
   },
   "id": "928c73a46e8ffd27"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "{4: array([   2,    3,    5,    7,    9,   12,   13,   19,   22,   24,   32,\n          33,   36,   41,   44,   45,   47,   52,   58,   62,   64,   66,\n          68,   74,   76,   82,   83,   84,   86,   98,   99,  103,  106,\n         115,  119,  126,  127,  128,  131,  134,  135,  142,  143,  151,\n         153,  156,  157,  158,  162,  178,  181,  182,  186,  200,  214,\n         220,  222,  227,  228,  229,  234,  235,  237,  239,  241,  245,\n         248,  249,  251,  255,  264,  268,  272,  276,  281,  284,  286,\n         288,  290,  307,  309,  311,  317,  318,  319,  321,  322,  327,\n         330,  339,  341,  346,  361,  363,  364,  366,  371,  374,  377,\n         388,  389,  391,  392,  393,  394,  403,  404,  408,  409,  410,\n         412,  418,  419,  421,  422,  423,  424,  428,  436,  446,  453,\n         456,  476,  479,  483,  484,  491,  493,  496,  503,  504,  512,\n         517,  518,  523,  530,  536,  538,  541,  543,  545,  547,  553,\n         554,  560,  561,  565,  570,  574,  580,  581,  587,  593,  594,\n         597,  598,  606,  609,  614,  616,  620,  621,  639,  642,  646,\n         656,  658,  659,  663,  665,  680,  684,  686,  688,  699,  720,\n         730,  745,  755,  763,  766,  770,  772,  777,  785,  791,  792,\n         793,  796,  798,  801,  803,  805,  809,  817,  821,  823,  824,\n         827,  828,  830,  831,  833,  843,  844,  853,  854,  858,  871,\n         879,  896,  901,  908,  910,  912,  918,  928,  931,  934,  938,\n         949,  950,  961,  965,  970,  971,  973,  974,  978,  987,  988,\n         992,  993,  997, 1021, 1032, 1034, 1042, 1044, 1050, 1051, 1055,\n        1056, 1062, 1068, 1075, 1097, 1108, 1111, 1120, 1122, 1126, 1131,\n        1132, 1140, 1146, 1149, 1150, 1156, 1159, 1163, 1166, 1168, 1189,\n        1190, 1191, 1196, 1200, 1205, 1215, 1223, 1231, 1232, 1233, 1234,\n        1241, 1247, 1250, 1257, 1259, 1262, 1273, 1280, 1288, 1293, 1298,\n        1299, 1300, 1302, 1309, 1323, 1333, 1334, 1338, 1342, 1343, 1349,\n        1350, 1357, 1363, 1367, 1368, 1370, 1371, 1372, 1384, 1385, 1389,\n        1398, 1407, 1410, 1411, 1418, 1424, 1429, 1431, 1434, 1435, 1444,\n        1446, 1467, 1470, 1480, 1483, 1487, 1494, 1504, 1512, 1524, 1527,\n        1530, 1531, 1534, 1537, 1539, 1548, 1556, 1557, 1558, 1561, 1563,\n        1564, 1573, 1578, 1581, 1585, 1586, 1599, 1600, 1616, 1618, 1620,\n        1621, 1625, 1633, 1636, 1641, 1643, 1644, 1668, 1676, 1679, 1684,\n        1688, 1689, 1692, 1693, 1694, 1696, 1699, 1705, 1711, 1715, 1717,\n        1720, 1722, 1725, 1731, 1734, 1737, 1742, 1744, 1747, 1758, 1759,\n        1766, 1767, 1768, 1772, 1781, 1789, 1790, 1793, 1798, 1803, 1814,\n        1819, 1827, 1842, 1868, 1883, 1884, 1889, 1896, 1897, 1898, 1899,\n        1911, 1915, 1923, 1930, 1933, 1942, 1945, 1950, 1952, 1958, 1971,\n        1974, 1976, 1980, 1990, 1992, 1995, 2001, 2004, 2007, 2008, 2017,\n        2019, 2020, 2022, 2023, 2034, 2035, 2039, 2043, 2054, 2062, 2066,\n        2067, 2072, 2074, 2082, 2087, 2089, 2092, 2097, 2098, 2105, 2115,\n        2118, 2119, 2128, 2132, 2145, 2150, 2164, 2169, 2173, 2183, 2184,\n        2189, 2193, 2198, 2201, 2209, 2212, 2217, 2219, 2232, 2234, 2236,\n        2237, 2239, 2243, 2245, 2250, 2251, 2254, 2257, 2259, 2260, 2270,\n        2272, 2273, 2276, 2279, 2281, 2284, 2287, 2294, 2305, 2311, 2313,\n        2314, 2317, 2320, 2322, 2325, 2336, 2338, 2342, 2344, 2350, 2355,\n        2361, 2364, 2365, 2375, 2378, 2380, 2383, 2385, 2393, 2395, 2405,\n        2408, 2419, 2421, 2422, 2423, 2424, 2447, 2449, 2453, 2458, 2463,\n        2480, 2486, 2500, 2501, 2505, 2507, 2511, 2515, 2516, 2520, 2528,\n        2540, 2544, 2551, 2552, 2569, 2573, 2579, 2606, 2607, 2608, 2612,\n        2618, 2622, 2624, 2629, 2633, 2637, 2640, 2642, 2649, 2651, 2652,\n        2656, 2661, 2662, 2664, 2666, 2668, 2672, 2674, 2682, 2685, 2692,\n        2695, 2696, 2698, 2699, 2706, 2709, 2716, 2721, 2723, 2727, 2734,\n        2738, 2741, 2744, 2747, 2754, 2759, 2761, 2762, 2765, 2767, 2771,\n        2784, 2786, 2802, 2805, 2806, 2811, 2816, 2818, 2822, 2824, 2831,\n        2838, 2843, 2847, 2849, 2861, 2862, 2871, 2878, 2879, 2889, 2891,\n        2896, 2898, 2909, 2921, 2924, 2930, 2931, 2933, 2936, 2948, 2949,\n        2953, 2959, 2963, 2968, 2973, 2978, 2982, 2984, 2993, 3002, 3011,\n        3021, 3027, 3032, 3048, 3050, 3054, 3055, 3059, 3063, 3067, 3068,\n        3069, 3071, 3082, 3083, 3085, 3107, 3126, 3132, 3136, 3140, 3145,\n        3147, 3153, 3154, 3160, 3163, 3166, 3170, 3175, 3181, 3192, 3193,\n        3194, 3195, 3201, 3203, 3204, 3218, 3221, 3226, 3227, 3231, 3232,\n        3242, 3250, 3262, 3267, 3268, 3277, 3278, 3283, 3286, 3297, 3302,\n        3317, 3320, 3321, 3327, 3332, 3333, 3334, 3344, 3355, 3356, 3358,\n        3380, 3382, 3389, 3398, 3400, 3401, 3402, 3406, 3409, 3422, 3423,\n        3424, 3425, 3433, 3434, 3438, 3442, 3448, 3454, 3466, 3470, 3479,\n        3486, 3487, 3492, 3499, 3501, 3503, 3523, 3542, 3546, 3547, 3552,\n        3556, 3557, 3561, 3563, 3565, 3571, 3579, 3586, 3587, 3591, 3598,\n        3604, 3605, 3606, 3609, 3611, 3621, 3622, 3623, 3625, 3628, 3632,\n        3635, 3646, 3648, 3656, 3658, 3665, 3667, 3673, 3676, 3678, 3680,\n        3682, 3684, 3690, 3704, 3709, 3710, 3715, 3717, 3719, 3725, 3728,\n        3733, 3734, 3735, 3737, 3738, 3744, 3756, 3760, 3762, 3763, 3771,\n        3773, 3776, 3779, 3782, 3798, 3804, 3810, 3823, 3828, 3829, 3833,\n        3844, 3846, 3847, 3851, 3853, 3865, 3870, 3873, 3878, 3880, 3881,\n        3886, 3892, 3893, 3898, 3899, 3902, 3907, 3910, 3914, 3921, 3923,\n        3926, 3930, 3931, 3937, 3942, 3951, 3952, 3958, 3959, 3964, 3966,\n        3970, 3978, 3982, 3986, 3998, 4006, 4009, 4010, 4012, 4021, 4024,\n        4026, 4040, 4049, 4051, 4057, 4061, 4064, 4068, 4069, 4088, 4104,\n        4106, 4126, 4130, 4132, 4133, 4136, 4143, 4144, 4151, 4159, 4162,\n        4163, 4169, 4174, 4176, 4178, 4182, 4183, 4188, 4189, 4194, 4199,\n        4202, 4204, 4208, 4215, 4218, 4225, 4226, 4228, 4232, 4248, 4251,\n        4257, 4261, 4264, 4270, 4279, 4280, 4283, 4285, 4289, 4294, 4306,\n        4308, 4309, 4323, 4328, 4335, 4339, 4343, 4359, 4364, 4371, 4375,\n        4381, 4387, 4390, 4391, 4394, 4400, 4403, 4422, 4423, 4424, 4426]),\n 6: array([   0,    8,  163,  193,  208,  219,  250,  348,  382,  383,  386,\n         420,  522,  542,  586,  601,  625,  668,  704,  717,  748,  753,\n         786,  841,  865,  886,  903,  911,  920,  921,  939,  962, 1012,\n        1014, 1022, 1026, 1039, 1082, 1085, 1096, 1117, 1124, 1151, 1153,\n        1183, 1187, 1228, 1230, 1238, 1245, 1275, 1285, 1290, 1301, 1322,\n        1352, 1373, 1381, 1393, 1401, 1420, 1438, 1443, 1460, 1463, 1481,\n        1498, 1526, 1541, 1583, 1584, 1596, 1606, 1610, 1611, 1617, 1619,\n        1680, 1706, 1776, 1882, 1901, 1909, 1943, 2029, 2070, 2091, 2113,\n        2161, 2178, 2185, 2274, 2295, 2309, 2327, 2328, 2340, 2346, 2363,\n        2389, 2425, 2437, 2470, 2496, 2517, 2538, 2541, 2558, 2568, 2581,\n        2588, 2623, 2626, 2645, 2655, 2667, 2768, 2798, 2853, 2892, 2957,\n        2966, 3006, 3015, 3039, 3092, 3093, 3177, 3213, 3241, 3249, 3251,\n        3318, 3326, 3349, 3369, 3375, 3419, 3439, 3460, 3473, 3494, 3533,\n        3549, 3590, 3626, 3695, 3724, 3740, 3793, 3809, 3817, 3832, 3843,\n        3845, 3861, 3903, 3904, 3945, 3975, 3983, 3989, 4023, 4063, 4123,\n        4203, 4227, 4238, 4252, 4262, 4269, 4275, 4281, 4284, 4301, 4342,\n        4345, 4347, 4374]),\n 7: array([  16,   21,   29,   31,   37,   38,   39,   61,   67,   87,   89,\n          91,   92,  118,  125,  132,  137,  150,  160,  166,  170,  175,\n         177,  183,  194,  195,  199,  201,  203,  209,  210,  215,  223,\n         231,  233,  246,  259,  271,  275,  277,  293,  296,  297,  301,\n         303,  304,  313,  337,  340,  350,  356,  369,  390,  402,  407,\n         433,  437,  444,  445,  463,  467,  472,  481,  511,  514,  519,\n         550,  566,  572,  585,  590,  592,  599,  602,  607,  610,  622,\n         628,  635,  643,  666,  687,  690,  693,  711,  714,  719,  725,\n         728,  736,  737,  740,  751,  761,  773,  779,  781,  799,  806,\n         816,  819,  822,  826,  835,  837,  842,  846,  851,  870,  877,\n         890,  891,  893,  894,  899,  923,  941,  942,  953,  979,  980,\n         996,  999, 1000, 1003, 1004, 1016, 1023, 1040, 1045, 1054, 1058,\n        1076, 1086, 1142, 1152, 1158, 1162, 1167, 1169, 1172, 1177, 1178,\n        1195, 1203, 1204, 1210, 1211, 1213, 1218, 1254, 1264, 1266, 1274,\n        1277, 1286, 1292, 1311, 1320, 1329, 1354, 1366, 1375, 1392, 1409,\n        1413, 1417, 1427, 1432, 1439, 1451, 1461, 1477, 1478, 1491, 1509,\n        1511, 1518, 1519, 1528, 1535, 1542, 1545, 1565, 1579, 1587, 1591,\n        1615, 1624, 1632, 1634, 1638, 1654, 1657, 1661, 1665, 1669, 1675,\n        1677, 1681, 1690, 1695, 1707, 1708, 1716, 1718, 1723, 1727, 1753,\n        1754, 1756, 1782, 1784, 1786, 1791, 1795, 1802, 1808, 1813, 1816,\n        1826, 1837, 1847, 1862, 1869, 1871, 1873, 1874, 1878, 1880, 1891,\n        1893, 1912, 1913, 1914, 1920, 1932, 1944, 1947, 1957, 1959, 1960,\n        1961, 1966, 1968, 1975, 1989, 1997, 2012, 2021, 2024, 2026, 2046,\n        2055, 2083, 2084, 2088, 2093, 2094, 2096, 2107, 2126, 2127, 2131,\n        2133, 2138, 2143, 2179, 2191, 2202, 2211, 2223, 2235, 2241, 2247,\n        2252, 2255, 2256, 2264, 2271, 2289, 2334, 2337, 2352, 2399, 2413,\n        2414, 2415, 2426, 2428, 2429, 2430, 2442, 2459, 2460, 2471, 2474,\n        2476, 2487, 2503, 2504, 2523, 2530, 2532, 2537, 2548, 2549, 2557,\n        2562, 2563, 2580, 2586, 2632, 2635, 2636, 2648, 2659, 2660, 2663,\n        2670, 2673, 2680, 2683, 2703, 2718, 2722, 2740, 2757, 2782, 2783,\n        2789, 2795, 2809, 2810, 2812, 2820, 2837, 2851, 2865, 2877, 2899,\n        2901, 2903, 2906, 2913, 2918, 2919, 2925, 2932, 2939, 2942, 2946,\n        2960, 2961, 2962, 2980, 2989, 3001, 3016, 3019, 3020, 3030, 3036,\n        3041, 3042, 3045, 3060, 3074, 3081, 3091, 3094, 3104, 3118, 3119,\n        3122, 3127, 3141, 3152, 3164, 3179, 3187, 3189, 3191, 3207, 3212,\n        3216, 3230, 3240, 3245, 3256, 3269, 3274, 3276, 3284, 3292, 3309,\n        3310, 3312, 3313, 3324, 3342, 3346, 3367, 3395, 3404, 3405, 3415,\n        3431, 3441, 3444, 3447, 3458, 3474, 3475, 3482, 3484, 3488, 3509,\n        3512, 3514, 3522, 3527, 3530, 3534, 3535, 3537, 3544, 3548, 3566,\n        3568, 3569, 3570, 3576, 3581, 3582, 3585, 3599, 3624, 3637, 3638,\n        3642, 3651, 3654, 3670, 3671, 3689, 3699, 3702, 3708, 3721, 3722,\n        3727, 3741, 3753, 3764, 3780, 3783, 3792, 3796, 3806, 3807, 3814,\n        3820, 3837, 3839, 3841, 3858, 3859, 3860, 3877, 3900, 3913, 3929,\n        3935, 3938, 3954, 3981, 3987, 3990, 4003, 4036, 4042, 4043, 4053,\n        4065, 4070, 4076, 4077, 4078, 4081, 4084, 4085, 4086, 4093, 4095,\n        4103, 4107, 4108, 4111, 4118, 4134, 4139, 4147, 4149, 4167, 4168,\n        4170, 4177, 4179, 4180, 4185, 4186, 4196, 4198, 4205, 4210, 4211,\n        4223, 4229, 4233, 4239, 4249, 4253, 4259, 4260, 4265, 4268, 4276,\n        4290, 4292, 4302, 4314, 4317, 4318, 4331, 4350, 4351, 4372, 4376,\n        4377, 4384, 4412, 4416]),\n 10: array([   1,   40,   48,   56,   65,   79,  111,  120,  146,  173,  174,\n         221,  225,  252,  256,  257,  269,  274,  355,  359,  373,  379,\n         380,  397,  425,  470,  485,  488,  506,  520,  531,  539,  557,\n         576,  579,  626,  627,  634,  648,  673,  678,  708,  726,  731,\n         734,  752,  771,  778,  780,  812,  848,  850,  852,  868,  872,\n         887,  957,  975,  982,  989, 1001, 1035, 1038, 1065, 1080, 1081,\n        1088, 1127, 1170, 1175, 1188, 1207, 1252, 1256, 1261, 1268, 1269,\n        1278, 1324, 1347, 1360, 1426, 1486, 1500, 1510, 1513, 1520, 1540,\n        1560, 1597, 1639, 1664, 1691, 1714, 1719, 1761, 1764, 1773, 1820,\n        1859, 1867, 1870, 1902, 1905, 1926, 1935, 2013, 2041, 2042, 2050,\n        2073, 2076, 2167, 2192, 2205, 2215, 2229, 2246, 2262, 2275, 2306,\n        2323, 2329, 2394, 2403, 2407, 2411, 2481, 2488, 2491, 2506, 2514,\n        2555, 2604, 2605, 2689, 2697, 2720, 2729, 2788, 2801, 2804, 2807,\n        2823, 2852, 2867, 2868, 2893, 2902, 2907, 2952, 2955, 2965, 2975,\n        2991, 3051, 3062, 3095, 3109, 3120, 3180, 3185, 3236, 3246, 3280,\n        3295, 3298, 3338, 3354, 3394, 3410, 3437, 3440, 3451, 3452, 3513,\n        3550, 3573, 3578, 3580, 3595, 3634, 3660, 3662, 3666, 3672, 3681,\n        3718, 3742, 3746, 3768, 3772, 3795, 3799, 3815, 3834, 3840, 3862,\n        3884, 3885, 3915, 4071, 4079, 4089, 4090, 4102, 4121, 4175, 4206,\n        4222, 4242, 4250, 4258, 4273, 4288, 4307, 4346, 4395, 4398, 4409,\n        4417, 4418, 4419]),\n 11: array([   6,   10,   18,   53,   75,  114,  130,  133,  140,  147,  148,\n         168,  172,  176,  191,  202,  226,  253,  254,  265,  299,  328,\n         329,  335,  372,  385,  400,  427,  434,  438,  460,  466,  475,\n         487,  489,  497,  498,  510,  521,  533,  549,  552,  558,  564,\n         568,  571,  577,  578,  584,  588,  596,  640,  647,  652,  657,\n         660,  692,  695,  716,  732,  739,  747,  759,  764,  810,  832,\n         857,  864,  874,  882,  885,  888,  895,  905,  909,  967,  968,\n         983, 1006, 1007, 1028, 1029, 1030, 1031, 1067, 1069, 1071, 1073,\n        1100, 1102, 1103, 1116, 1119, 1125, 1128, 1133, 1161, 1176, 1180,\n        1181, 1201, 1219, 1243, 1246, 1251, 1255, 1258, 1270, 1308, 1321,\n        1332, 1364, 1396, 1428, 1437, 1452, 1459, 1468, 1472, 1495, 1517,\n        1546, 1566, 1582, 1589, 1607, 1609, 1631, 1637, 1651, 1736, 1743,\n        1749, 1760, 1794, 1797, 1805, 1806, 1807, 1830, 1839, 1844, 1848,\n        1852, 1853, 1892, 1916, 1925, 1948, 1949, 1954, 1962, 1963, 1978,\n        1984, 1991, 1999, 2000, 2002, 2006, 2028, 2031, 2032, 2056, 2058,\n        2060, 2079, 2103, 2111, 2125, 2144, 2147, 2160, 2186, 2214, 2216,\n        2220, 2225, 2253, 2263, 2265, 2332, 2335, 2353, 2358, 2366, 2374,\n        2376, 2379, 2386, 2390, 2402, 2404, 2412, 2438, 2450, 2469, 2477,\n        2479, 2482, 2492, 2499, 2510, 2527, 2531, 2554, 2559, 2583, 2585,\n        2595, 2598, 2614, 2641, 2644, 2650, 2681, 2714, 2726, 2748, 2755,\n        2758, 2779, 2780, 2785, 2792, 2793, 2835, 2850, 2860, 2866, 2874,\n        2912, 2914, 2928, 2940, 2944, 2967, 2990, 2995, 2996, 3009, 3014,\n        3026, 3034, 3044, 3064, 3066, 3076, 3080, 3096, 3112, 3114, 3121,\n        3133, 3146, 3162, 3167, 3171, 3200, 3210, 3233, 3234, 3243, 3253,\n        3272, 3306, 3314, 3316, 3336, 3363, 3381, 3397, 3403, 3408, 3417,\n        3449, 3450, 3462, 3476, 3478, 3489, 3493, 3519, 3531, 3539, 3545,\n        3551, 3554, 3610, 3669, 3687, 3692, 3700, 3707, 3770, 3774, 3784,\n        3785, 3786, 3790, 3825, 3866, 3871, 3889, 3950, 3953, 3973, 3976,\n        3979, 3991, 3999, 4029, 4056, 4099, 4105, 4112, 4124, 4128, 4129,\n        4152, 4158, 4192, 4255, 4354, 4356, 4362, 4368, 4373, 4396, 4406,\n        4414, 4429]),\n 12: array([  77,  112,  154,  159,  169,  187,  315,  324,  332,  353,  358,\n         381,  429,  452,  502,  528,  604,  623,  685,  712,  723,  750,\n         757,  768,  776,  825,  859,  867,  883,  902,  929,  984,  985,\n        1046, 1083, 1113, 1118, 1141, 1143, 1214, 1260, 1295, 1339, 1361,\n        1412, 1473, 1515, 1521, 1601, 1650, 1659, 1670, 1709, 1769, 1771,\n        1779, 1804, 1855, 1860, 1985, 1987, 2057, 2078, 2137, 2158, 2159,\n        2182, 2194, 2195, 2249, 2258, 2303, 2307, 2359, 2406, 2550, 2576,\n        2593, 2620, 2742, 2796, 2808, 2828, 2895, 2937, 2938, 3047, 3084,\n        3097, 3111, 3168, 3263, 3315, 3330, 3453, 3468, 3491, 3594, 3608,\n        3629, 3630, 3644, 3668, 3686, 3749, 3759, 3819, 3824, 3852, 3909,\n        3927, 3948, 3961, 3974, 3994, 3997, 4005, 4020, 4030, 4052, 4058,\n        4060, 4113, 4140, 4207, 4234, 4267, 4271, 4272, 4326]),\n 15: array([  14,   23,   27,   30,   46,   54,   70,   71,   73,   81,   94,\n          95,  108,  123,  124,  155,  185,  196,  205,  212,  213,  218,\n         224,  230,  243,  258,  260,  270,  273,  278,  283,  285,  287,\n         291,  292,  294,  295,  314,  316,  326,  338,  344,  351,  352,\n         357,  360,  370,  378,  384,  387,  398,  401,  406,  417,  455,\n         457,  459,  462,  465,  474,  478,  490,  499,  505,  513,  515,\n         529,  534,  546,  563,  575,  591,  595,  605,  608,  612,  617,\n         631,  632,  649,  653,  654,  661,  662,  667,  669,  674,  689,\n         701,  702,  703,  705,  713,  721,  724,  727,  733,  735,  738,\n         746,  754,  767,  769,  788,  790,  807,  814,  818,  840,  847,\n         849,  861,  863,  866,  869,  873,  876,  884,  915,  917,  924,\n         925,  926,  933,  937,  940,  944,  948,  963,  964,  966,  976,\n         998, 1010, 1015, 1020, 1024, 1048, 1059, 1066, 1072, 1095, 1107,\n        1109, 1114, 1121, 1135, 1138, 1139, 1147, 1154, 1164, 1165, 1186,\n        1192, 1194, 1199, 1202, 1208, 1209, 1217, 1220, 1221, 1227, 1236,\n        1242, 1244, 1248, 1249, 1253, 1263, 1271, 1279, 1287, 1291, 1296,\n        1312, 1313, 1317, 1319, 1327, 1331, 1340, 1341, 1344, 1345, 1346,\n        1358, 1362, 1365, 1369, 1374, 1376, 1382, 1383, 1388, 1391, 1394,\n        1399, 1402, 1405, 1416, 1430, 1436, 1448, 1454, 1458, 1464, 1469,\n        1471, 1484, 1485, 1488, 1489, 1493, 1501, 1522, 1523, 1529, 1532,\n        1536, 1543, 1544, 1550, 1552, 1553, 1554, 1555, 1568, 1576, 1588,\n        1602, 1608, 1622, 1623, 1628, 1629, 1630, 1640, 1647, 1655, 1662,\n        1683, 1686, 1697, 1698, 1701, 1704, 1721, 1724, 1726, 1728, 1732,\n        1739, 1751, 1752, 1757, 1774, 1778, 1783, 1799, 1801, 1811, 1812,\n        1815, 1823, 1840, 1849, 1856, 1861, 1865, 1881, 1894, 1895, 1904,\n        1906, 1922, 1924, 1927, 1931, 1936, 1941, 1964, 1965, 1970, 1972,\n        1973, 1981, 1986, 2009, 2014, 2016, 2030, 2033, 2037, 2044, 2048,\n        2053, 2065, 2071, 2080, 2095, 2108, 2110, 2114, 2124, 2139, 2142,\n        2148, 2149, 2152, 2155, 2157, 2162, 2165, 2168, 2170, 2177, 2196,\n        2200, 2208, 2210, 2218, 2222, 2224, 2231, 2242, 2244, 2267, 2268,\n        2278, 2280, 2282, 2286, 2290, 2291, 2302, 2308, 2312, 2333, 2341,\n        2349, 2354, 2369, 2370, 2371, 2377, 2381, 2387, 2388, 2400, 2410,\n        2427, 2434, 2441, 2454, 2457, 2461, 2462, 2467, 2473, 2478, 2484,\n        2485, 2489, 2490, 2493, 2497, 2498, 2502, 2513, 2521, 2522, 2535,\n        2539, 2543, 2545, 2564, 2565, 2567, 2577, 2590, 2596, 2599, 2601,\n        2602, 2609, 2611, 2619, 2627, 2628, 2643, 2653, 2654, 2657, 2671,\n        2675, 2679, 2684, 2686, 2693, 2700, 2701, 2708, 2710, 2711, 2728,\n        2730, 2731, 2733, 2737, 2750, 2756, 2760, 2770, 2773, 2781, 2790,\n        2791, 2797, 2813, 2827, 2829, 2832, 2834, 2836, 2840, 2845, 2857,\n        2858, 2875, 2876, 2880, 2882, 2884, 2885, 2887, 2904, 2910, 2923,\n        2943, 2945, 2947, 2956, 2974, 2979, 2986, 2988, 2994, 2997, 3003,\n        3007, 3008, 3010, 3013, 3024, 3025, 3037, 3038, 3040, 3065, 3073,\n        3087, 3088, 3089, 3099, 3102, 3113, 3123, 3124, 3135, 3148, 3156,\n        3157, 3158, 3165, 3173, 3176, 3182, 3196, 3202, 3217, 3222, 3235,\n        3238, 3239, 3244, 3247, 3257, 3260, 3261, 3265, 3266, 3271, 3275,\n        3281, 3288, 3291, 3293, 3299, 3301, 3305, 3325, 3328, 3331, 3340,\n        3352, 3353, 3360, 3361, 3362, 3368, 3370, 3371, 3386, 3393, 3399,\n        3418, 3426, 3427, 3429, 3445, 3455, 3457, 3459, 3461, 3465, 3471,\n        3483, 3490, 3505, 3506, 3507, 3511, 3515, 3516, 3521, 3524, 3526,\n        3528, 3540, 3553, 3555, 3558, 3560, 3572, 3575, 3603, 3613, 3616,\n        3619, 3639, 3640, 3641, 3653, 3655, 3657, 3659, 3661, 3664, 3674,\n        3683, 3694, 3714, 3720, 3723, 3730, 3739, 3745, 3750, 3751, 3752,\n        3757, 3761, 3777, 3794, 3797, 3803, 3805, 3811, 3812, 3826, 3850,\n        3855, 3856, 3857, 3863, 3867, 3868, 3872, 3875, 3876, 3882, 3883,\n        3897, 3905, 3918, 3920, 3922, 3924, 3932, 3933, 3939, 3947, 3949,\n        3955, 3960, 3980, 3985, 3995, 4004, 4008, 4011, 4015, 4025, 4027,\n        4034, 4037, 4038, 4044, 4050, 4054, 4055, 4062, 4091, 4096, 4098,\n        4109, 4110, 4114, 4115, 4117, 4125, 4127, 4131, 4135, 4138, 4141,\n        4154, 4157, 4164, 4166, 4171, 4172, 4173, 4184, 4187, 4190, 4197,\n        4213, 4217, 4224, 4231, 4235, 4240, 4243, 4245, 4247, 4263, 4266,\n        4278, 4286, 4291, 4295, 4297, 4298, 4303, 4304, 4310, 4315, 4321,\n        4325, 4334, 4340, 4341, 4348, 4352, 4353, 4357, 4366, 4379, 4380,\n        4383, 4386, 4388, 4389, 4393, 4407, 4408, 4411, 4420, 4425]),\n 16: array([  93,  116,  136,  180,  184,  207,  279,  331,  368,  426,  441,\n         448,  473,  526,  589,  615,  624,  682,  762,  774,  782,  783,\n         811,  815,  916,  927,  969,  972, 1019, 1049, 1052, 1053, 1093,\n        1123, 1160, 1182, 1212, 1267, 1272, 1304, 1310, 1326, 1359, 1408,\n        1440, 1456, 1502, 1549, 1577, 1605, 1645, 1649, 1713, 1755, 1765,\n        1788, 1792, 1809, 1822, 1857, 1864, 1875, 1900, 1938, 1953, 1983,\n        2038, 2051, 2052, 2077, 2104, 2203, 2233, 2248, 2304, 2326, 2331,\n        2343, 2483, 2508, 2509, 2524, 2534, 2560, 2591, 2603, 2704, 2705,\n        2715, 2719, 2753, 2908, 3035, 3072, 3086, 3098, 3106, 3131, 3142,\n        3229, 3264, 3289, 3294, 3412, 3421, 3436, 3469, 3472, 3495, 3567,\n        3607, 3633, 3650, 3675, 3698, 3736, 3754, 3767, 3775, 3822, 3888,\n        3965, 4032, 4072, 4080, 4165, 4236, 4313, 4316, 4319, 4322, 4329,\n        4370, 4405, 4415]),\n 17: array([  15,   25,   42,   51,   80,  109,  122,  188,  189,  192,  236,\n         261,  263,  298,  306,  375,  396,  399,  449,  492,  494,  500,\n         507,  509,  532,  537,  555,  559,  562,  569,  600,  603,  619,\n         679,  691,  718,  741,  749,  758,  760,  787,  789,  813,  839,\n         892,  904,  906,  913,  922,  935,  951,  955,  977, 1002, 1043,\n        1057, 1060, 1078, 1090, 1091, 1106, 1115, 1144, 1145, 1148, 1173,\n        1179, 1206, 1222, 1276, 1289, 1351, 1353, 1355, 1379, 1406, 1415,\n        1422, 1433, 1496, 1525, 1559, 1571, 1627, 1656, 1674, 1745, 1763,\n        1775, 1818, 1828, 1829, 1838, 1841, 1843, 1845, 1854, 1866, 1877,\n        1886, 1907, 1929, 2018, 2027, 2049, 2116, 2120, 2141, 2199, 2213,\n        2227, 2269, 2285, 2292, 2293, 2330, 2392, 2397, 2418, 2494, 2533,\n        2561, 2584, 2610, 2613, 2617, 2745, 2794, 2803, 2841, 2842, 2844,\n        2897, 2905, 2922, 2941, 2950, 2970, 3005, 3022, 3115, 3137, 3144,\n        3155, 3159, 3188, 3197, 3205, 3209, 3215, 3223, 3228, 3237, 3270,\n        3273, 3335, 3350, 3373, 3379, 3383, 3384, 3388, 3392, 3396, 3407,\n        3416, 3463, 3477, 3497, 3502, 3529, 3559, 3618, 3620, 3643, 3697,\n        3705, 3726, 3755, 3789, 3821, 3842, 3906, 3916, 3928, 3934, 3963,\n        3977, 4000, 4014, 4039, 4048, 4066, 4094, 4146, 4160, 4200, 4212,\n        4230, 4254, 4274, 4293, 4311, 4324, 4337, 4360, 4363, 4378, 4382,\n        4392]),\n 18: array([   4,   11,   17, ..., 4421, 4427, 4428])}"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaf_samples_indices"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T09:09:51.036697Z",
     "start_time": "2024-08-07T09:09:51.005197Z"
    }
   },
   "id": "842b69204fdb5835"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Load test accuracies per leaf for the blackbox x->y model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b81c798567e474f"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "output_path = \"/Users/gouse/PycharmProjects/AR-Imperial-Thesis/logs_and_models_to_show/analysis_notebooks/completeness_scores/blackbox_model/test_pred_correct_or_not_xtoy.pkl\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T09:09:51.037073Z",
     "start_time": "2024-08-07T09:09:51.008682Z"
    }
   },
   "id": "4092fb8253673f1e"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of test dataset:  4430\n",
      "Accuracy of the blackbox model:  99.50338745117188\n"
     ]
    }
   ],
   "source": [
    "# open pickle file\n",
    "with open(output_path, 'rb') as f:\n",
    "    test_pred_correct_or_not_xtoy = pkl.load(f)\n",
    "\n",
    "print(\"Length of test dataset: \", len(test_pred_correct_or_not_xtoy))\n",
    "acc = test_pred_correct_or_not_xtoy.sum() * 100/len(test_pred_correct_or_not_xtoy)\n",
    "print(\"Accuracy of the blackbox model: \", acc.item())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T09:09:51.037421Z",
     "start_time": "2024-08-07T09:09:51.011616Z"
    }
   },
   "id": "bfc56b849bd37344"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "{4: 0.9978355169296265,\n 6: 1.0,\n 7: 0.9963167309761047,\n 10: 1.0,\n 11: 0.990963876247406,\n 12: 1.0,\n 15: 0.9942196607589722,\n 16: 1.0,\n 17: 0.9952380657196045,\n 18: 0.9905837774276733}"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute the acuracy of the blackbox model per leaf\n",
    "accuracy_per_path_blackbox_dict = {}\n",
    "for path in leaf_samples_indices.keys():\n",
    "    indices = leaf_samples_indices[path]\n",
    "    accuracy_per_path_blackbox_dict[path] = (test_pred_correct_or_not_xtoy[indices].sum()/len(indices)).item()\n",
    "    \n",
    "accuracy_per_path_blackbox_dict"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T09:09:51.037723Z",
     "start_time": "2024-08-07T09:09:51.015458Z"
    }
   },
   "id": "9ff1ab02ef8e9595"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Compute the test completeness scores"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b44ad6733726ea50"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "acuracy_of_random_guessing = 1/3\n",
    "\n",
    "completeness_scores_per_original_path = {}\n",
    "for path in accuracy_per_original_path_dict.keys():\n",
    "    completeness_scores_per_original_path[path] = (accuracy_per_original_path_dict[path] - acuracy_of_random_guessing) / (accuracy_per_path_blackbox_dict[path]  - acuracy_of_random_guessing)\n",
    "    \n",
    "completeness_scores_per_new_path = {}\n",
    "for path in accuracy_per_new_path_dict.keys():\n",
    "    completeness_scores_per_new_path[path] = (accuracy_per_new_path_dict[path] - acuracy_of_random_guessing) / (accuracy_per_path_blackbox_dict[path]  - acuracy_of_random_guessing)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T09:09:51.037950Z",
     "start_time": "2024-08-07T09:09:51.019824Z"
    }
   },
   "id": "89d5a4f444c6556d"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original path: 4, Completeness score: 0.34364819859450074\n",
      "Original path: 6, Completeness score: 0.3966480446927374\n",
      "Original path: 7, Completeness score: 0.3277777915080027\n",
      "Original path: 10, Completeness score: 0.179372197309417\n",
      "Original path: 11, Completeness score: 0.19389312363082153\n",
      "Original path: 12, Completeness score: 0.4576923076923076\n",
      "Original path: 15, Completeness score: 0.2762390638871759\n",
      "Original path: 16, Completeness score: 0.5666666666666667\n",
      "Original path: 17, Completeness score: 0.9784173098208234\n",
      "Original path: 18, Completeness score: 0.6676218036413173\n",
      "New path: 4, Completeness score: 0.3843648098023801\n",
      "New path: 6, Completeness score: 0.3966480446927374\n",
      "New path: 7, Completeness score: 0.3277777915080027\n",
      "New path: 10, Completeness score: 0.179372197309417\n",
      "New path: 11, Completeness score: 0.19389312363082153\n",
      "New path: 12, Completeness score: 0.4576923076923076\n",
      "New path: 15, Completeness score: 0.3090378973302443\n",
      "New path: 16, Completeness score: 0.5666666666666667\n",
      "New path: 17, Completeness score: 0.9784173098208234\n",
      "New path: 18, Completeness score: 0.7206304017845121\n"
     ]
    }
   ],
   "source": [
    "for path in completeness_scores_per_original_path.keys():\n",
    "    print(f\"Original path: {path}, Completeness score: {completeness_scores_per_original_path[path]}\")\n",
    "    \n",
    "for path in completeness_scores_per_new_path.keys():\n",
    "    print(f\"New path: {path}, Completeness score: {completeness_scores_per_new_path[path]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T09:09:51.038177Z",
     "start_time": "2024-08-07T09:09:51.023253Z"
    }
   },
   "id": "5bc9efe4e54fb3ed"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T09:09:51.038246Z",
     "start_time": "2024-08-07T09:09:51.025800Z"
    }
   },
   "id": "d1b859925407af73"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
